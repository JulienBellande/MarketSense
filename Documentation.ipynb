{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MarketSense\n",
    "\n",
    "**MarketSense** simule le rôle d’un **Data Engineer Analytics** spécialisé en IA. L’objectif est de créer un **dashboard d’aide à la décision** pour des investisseurs intraday et long terme. Le projet comprend des **pipelines de traitement des données**, un modèle **GRU** pour prédire les marchés financiers, et une application **Streamlit** pour la visualisation.\n",
    "\n",
    "Le code est conçu avec des **classes Python** et suit une approche orientée objet pour maintenir une bonne scalabilité et modularité.\n",
    "\n",
    "---\n",
    "\n",
    "## 🌐 Architecture du projet\n",
    "\n",
    "```\n",
    "├── BigQuery\n",
    "│   └── Database (BigQuery Dataset)\n",
    "│       └── News_Data\n",
    "├── Cloud Storage\n",
    "│   └── Wallet Data (CSV)\n",
    "├── GRU_Agent.keras\n",
    "├── GruAgent.py\n",
    "├── IA_research.ipynb\n",
    "├── Pipeline\n",
    "│   ├── core/\n",
    "│   │   └── main_pipeline.py\n",
    "│   ├── market_data/\n",
    "│   │   └── PipeMarketData.py\n",
    "│   ├── news_data/\n",
    "│   │   └── PipeNewsData.py\n",
    "│   ├── sentiment_data/\n",
    "│   │   └── PipeSentimentData.py\n",
    "│   ├── storage/\n",
    "│   │   └── StorageData.py\n",
    "│   └── wallet_data/\n",
    "│       └── PipeWalletData.py\n",
    "├── graph.py\n",
    "├── main.py\n",
    "└── README.md\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Pipelines\n",
    "\n",
    "Les pipelines suivent des **bonnes pratiques d’entreprise** pour le traitement des données, garantissant **scalabilité** et **efficacité**.\n",
    "\n",
    "- **Extraction** des données, **transformation**, et **stockage** dans **BigQuery** ou **Cloud Storage**.\n",
    "- Les données sont uniformisées pour un usage en **heure européenne**.\n",
    "- L’alimentation des données peut être automatisée pour être **continue** en entreprise.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Données financières – `market_data`\n",
    "\n",
    "Les données proviennent de **Yahoo Finance** via l’API Python `yfinance`.\n",
    "\n",
    "- **Features ajoutées :**\n",
    "  - `SMA_50` : Moyenne mobile sur 50 périodes.\n",
    "  - `Volume_Spike` : Détection de pics d’activité (volume supérieur à deux écarts-types).\n",
    "\n",
    "⚠️ Pour du **temps réel**, une API payante est nécessaire.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Sentiment de marché – `sentiment_data`\n",
    "\n",
    "Les données de sentiment proviennent de l'**API CNN Business**.  \n",
    "Cet indicateur mesure la **peur ou la confiance** des investisseurs. Il reflète souvent un **effet boule de neige** dans le comportement des marchés.\n",
    "\n",
    "---\n",
    "\n",
    "## 📰 Infos économiques – `news_data`\n",
    "\n",
    "Les informations économiques sont extraites d’un **flux RSS** en temps réel.\n",
    "\n",
    "Ces données sont ensuite transformées et stockées dans **BigQuery**, permettant de récupérer facilement les 5 dernières actualités économiques.\n",
    "\n",
    "---\n",
    "\n",
    "## 💼 Données de portefeuille – `wallet_data`\n",
    "\n",
    "Les investisseurs peuvent fournir un **fichier CSV** de leurs positions (exemple avec `Wallet.csv`). Ce fichier est stocké dans un **Bucket Google Cloud Storage**.\n",
    "\n",
    "Le pipeline :\n",
    "- Calcule le **prix moyen d’achat** par actif.\n",
    "- Calcule la **variation actuelle** du portefeuille (% de gain/perte).\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 GRU Agent – `GruAgent.py`\n",
    "\n",
    "Après des expérimentations dans le notebook `IA_research.ipynb`, un modèle performant a été sauvegardé (`GRU_Agent.keras`).\n",
    "\n",
    "- **Pourquoi GRU ?**\n",
    "  - Plus rapide et moins gourmand que le LSTM.\n",
    "  - Excellente performance en temps réel pour les prédictions financières à haute fréquence.\n",
    "\n",
    "Les **GRU** sont des variantes des RNN, largement utilisés dans le **Deep Learning** pour la prévision des séries temporelles.\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Visualisation – `graph.py` & `Streamlit`\n",
    "\n",
    "Les visualisations des données sont réalisées via **Streamlit**. Le fichier `graph.py` contient des classes pour simplifier la construction des graphiques et la gestion de l'affichage.\n",
    "\n",
    "💡 Ce type de dashboard pourrait facilement être reproduit avec des outils comme **Power BI** ou **Tableau**, largement utilisés en entreprise pour la visualisation des données.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Main – `main.py`\n",
    "\n",
    "Le fichier `main.py` est le **point d’entrée** de l’application **Streamlit**.  \n",
    "Il orchestre :\n",
    "\n",
    "- La **mise à jour automatique** des données (chaque heure entre 13h30 et 21h30, horaires de la bourse US).\n",
    "- L'exécution des **pipelines de traitement des données**.\n",
    "- L'affichage interactif du **dashboard**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ Stockage des données et gestion sur GCP\n",
    "\n",
    "1. **BigQuery** :  \n",
    "   BigQuery est utilisé pour le **stockage des données transformées**. Cela inclut les données de **sentiment**, **actualités**, et les résultats des prédictions du modèle. La gestion de la base de données est facilitée grâce à BigQuery pour un stockage et une requêtabilité rapide et scalable.\n",
    "\n",
    "2. **Google Cloud Storage (Buckets)** :  \n",
    "   Les données du portefeuille des utilisateurs sont stockées sous forme de **fichiers CSV** dans des **Buckets Google Cloud Storage**. Ce stockage est utilisé pour la gestion des fichiers volumineux (comme les fichiers CSV) et est directement accessible via des pipelines pour les transformer avant de les stocker dans BigQuery ou d'autres bases de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
